{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import gzip\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import itertools\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import keras\n",
    "from PIL import Image\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import optimizers\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from scipy import stats\n",
    "from statistics import mode \n",
    "import scipy\n",
    "import statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingDataFile = 'mnist.pkl.gz'\n",
    "Data = gzip.open(TrainingDataFile,'rb')\n",
    "training,validation,testing= pickle.load(Data,encoding='latin1')\n",
    "trainingData = training[0]\n",
    "trainingTarget = training[1]\n",
    "validationData = validation[0]\n",
    "validationTarget = validation[1]\n",
    "testingData = testing[0]\n",
    "testingTarget = testing[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.zeros(shape=(50000,10))\n",
    "for x in range(len(trainingData)):\n",
    "    t[x][trainingTarget[x]] = 1\n",
    "v = np.zeros(shape=(10000,10))\n",
    "for x in range(len(validationData)):\n",
    "    v[x][validationTarget[x]] = 1\n",
    "te = np.zeros(shape=(10000,10))\n",
    "for x in range(len(testingData)):\n",
    "    te[x][testingTarget[x]] = 1\n",
    "\n",
    "def calculateSoftmax(x):\n",
    "  return (np.exp(x)/np.sum(np.exp(x)))\n",
    "\n",
    "def prediction(data,weights):\n",
    "    return(calculateSoftmax(np.dot(data,weights)))\n",
    "\n",
    "def calculateErrorFunction(weights,target,data):\n",
    "    predict = prediction(data,weights)\n",
    "    error = (-1/len(target))*np.sum(np.multiply(target,np.log(predict)))\n",
    "    return error\n",
    "\n",
    "def probability(data,weights):\n",
    "    return np.argmax(calculateSoftmax((data.T*weights)),axis=1)\n",
    "\n",
    "W_Now = np.random.randn(trainingData.shape[1],10)*0.05\n",
    "errorList = []\n",
    "Val = []\n",
    "TR = []\n",
    "Test = []\n",
    "TrainingAcc = []\n",
    "ValidationAcc = []\n",
    "TestingAcc = []\n",
    "learningRate =0.001\n",
    "index = 0\n",
    "\n",
    "\n",
    "def getWeights(W_Now,iteration):\n",
    "    for i in range(iteration):\n",
    "        predict = prediction(trainingData,W_Now)\n",
    "        gradient = np.dot(trainingData.T,predict-t)\n",
    "        gradient = gradient/len(t)\n",
    "        W_NEXT = W_Now - (learningRate * gradient)\n",
    "        W_Now = W_NEXT\n",
    "    return W_Now\n",
    "W_Now = getWeights(W_Now,300)\n",
    "W_Now = np.matrix(W_Now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForest = RandomForestClassifier(n_estimators = 20,n_jobs=1,criterion = 'entropy')\n",
    "randomForest.fit(trainingData,trainingTarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.01, kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(kernel = 'linear',gamma=0.01)\n",
    "svm.fit(trainingData,trainingTarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "(MNISTtrainingData,MNISTtrainingTarget),(MNISTtestingData,MNISTtestingTarget) = mnist.load_data()\n",
    "MNISTtrainingData = MNISTtrainingData.reshape(MNISTtrainingData.shape[0],784)\n",
    "MNISTtestingData = MNISTtestingData.reshape(MNISTtestingData.shape[0],784)\n",
    "MNISTtrainingTarget = tf.keras.utils.to_categorical(MNISTtrainingTarget,10)\n",
    "MNISTtestingTarget = tf.keras.utils.to_categorical(MNISTtestingTarget,10)\n",
    "Model =Sequential()\n",
    "Model.add(Dense(units=32,activation='relu',input_shape=(784,)))\n",
    "Model.add(Dense(units=32,activation='relu',input_shape=(784,)))\n",
    "Model.add(Dense(units=10,activation='softmax'))\n",
    "Model.compile(optimizer ='RMSprop',loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "history = Model.fit(MNISTtrainingData,MNISTtrainingTarget,batch_size= 128,epochs = 30,verbose=False, validation_split= 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Four Models( From Scratch ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuray is: 95.0 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "from collections import Counter\n",
    "prediction = []\n",
    "c = 0\n",
    "iteration = 100\n",
    "for y in range(iteration):\n",
    "    index = np.random.choice(len(testingData), 1, replace=False)\n",
    "    #print(index)\n",
    "    testingDataSample = [testingData[i] for i in index]\n",
    "    testingTargetSample = [testingTarget[i] for i in index]\n",
    "    randomForestAcc = randomForest.predict(testingDataSample)    #predict the target use random forest\n",
    "    RF = randomForestAcc\n",
    "    NNDataarray = np.array(testingDataSample)\n",
    "    NNTargetarray = np.array(testingTargetSample)\n",
    "    NNtestingData =  NNDataarray.reshape( NNDataarray.shape[0],784)\n",
    "    neuralNetworkAcc = Model.predict(NNtestingData,verbose = False)   \n",
    "    neuralNetworkAcc = list(neuralNetworkAcc)\n",
    "    NN = []\n",
    "    NN = np.array(NN)\n",
    "    for x in range(0,len(neuralNetworkAcc)):\n",
    "        maxIndex = np.max(neuralNetworkAcc[x])\n",
    "        NN = np.append(NN,np.where(neuralNetworkAcc[x]==maxIndex))      #predict the target use neural network\n",
    "    NN = NN.astype(int)                                          \n",
    "    SVM = svm.predict(testingDataSample)\n",
    "    LRtestingDataSample = np.array(testingDataSample)\n",
    "    LR = []\n",
    "    LR = np.array(LR)\n",
    "    for x in range(0,len(LRtestingDataSample)):\n",
    "        LR = np.append(LR ,probability(LRtestingDataSample[x].T,W_Now))    # #predict the target use logistic regression\n",
    "    LR  = LR.astype(int)\n",
    "    compareList = []\n",
    "    compareList.append(RF[x])  \n",
    "    compareList.append(SVM[x])\n",
    "    compareList.append(NN[x])\n",
    "    compareList.append(LR[x])                # append all the result getting from 4 classifiers into the list\n",
    "    numberElements = Counter(compareList)\n",
    "    maximum = max(numberElements.values())\n",
    "    mode = [j for j,z in numberElements.items() if z == maximum]\n",
    "    prediction = (mode[0])               # pick the most common value into the prediction list, if they have even\n",
    "    c +=1                                    # number, then pick the first occur common value\n",
    "    if prediction == testingTargetSample:\n",
    "        correct+=1\n",
    "\n",
    "accuracy = (correct/iteration)*100\n",
    "print(\"The accuray is:\",accuracy,'%')\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
